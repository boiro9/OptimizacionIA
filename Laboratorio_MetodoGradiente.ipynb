{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35293786",
   "metadata": {},
   "source": [
    "# üßÆ Laboratorio M√©todo del Gradiente\n",
    "\n",
    "[![Open in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/boiro9/OptimizacionIA/main?filepath=Laboratorio_MetodoGradiente.ipynb)\n",
    "\n",
    "En esta pr√°ctica vamos a implementar en Python el m√©todo del **gradiente** y m√©todo del **gradiente estoc√°stico**. Lo aplicaremos a distintos problemas analizando su comportamiento en funci√≥n de distintos par√°metros que definen el algoritmo, principalmente analizando el efecto del **tama√±o del paso**. Adem√°s, lo adaptaremos para resolver un ajuste de **regresi√≥n lineal**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f68fce-b713-402a-ac9e-aa276a4eac51",
   "metadata": {},
   "source": [
    "# üöÄ  Instalaci√≥n de paquetes:\n",
    "\n",
    "Los requerimientos para poder ejecutar este jupyter son los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c50ab0a-d70d-4072-ac6a-3ef0e4ab3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar numpy, plotly, pandas y sklearn\n",
    "!pip install numpy plotly pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95fb22b-0838-43dc-8143-b9fd8cc6dfa6",
   "metadata": {},
   "source": [
    "# 1. M√©todo del gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfead57",
   "metadata": {},
   "source": [
    "Supongamos que queremos aplicar el **m√©todo del gradiente** para minimizar la siguiente funci√≥n:\n",
    "$$\n",
    "\\min_{x}f(x)=x^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5f1b",
   "metadata": {},
   "source": [
    "Implementemos el m√©todo del gradiente para este caso:\n",
    "\n",
    "- **Inicializaci√≥n.** Elegir $\\varepsilon >0$. Elegir un punto inicial $\\pmb{x}^1$. Definir $t=1$.\n",
    "- **Paso 1.**  \n",
    "    -  Si $||\\nabla f(\\pmb{x}^t)||<\\varepsilon$, **stop**. Devolvemos $\\pmb{x}^t$.\n",
    "    -  En otro caso, definir $\\pmb{d}^t=-\\nabla f(\\pmb{x}^t)$ y tama√±o del paso $\\lambda^t$:\n",
    "\t\t$$\\pmb{x}^{t+1}=\\pmb{x}^t+\\lambda^t\\pmb{d}^t$$\n",
    "\t\tReemplazar $t$ por $t+1$. Repetir el **Paso 1**.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c7ea3",
   "metadata": {},
   "source": [
    "Notemos que, para este ejemplo concreto tenemos que:\n",
    "- $x\\in \\mathbb{R}$\n",
    "- $\\nabla f(x) = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6112a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def metodo_gradiente(xini,paso,n_iter=50,eps=1e-06):\n",
    "\n",
    "    xiter = [xini]\n",
    "    xold = xini\n",
    "    for it in range(n_iter):\n",
    "        gradf = 2*xold\n",
    "        if np.abs(gradf)<= eps:\n",
    "            print('Convergencia alcanzada en iter: ',it)\n",
    "            break\n",
    "        d = -gradf\n",
    "        xnew = xold + paso*d\n",
    "        xiter.append(xnew)\n",
    "        xold = xnew\n",
    "\n",
    "    if it == n_iter - 1:\n",
    "        print(f'Advertencia: Se alcanz√≥ el n√∫mero m√°ximo de iteraciones ({n_iter}) sin convergencia.')\n",
    "    \n",
    "    return xiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077b310",
   "metadata": {},
   "source": [
    "Aplicamos el m√©todo del gradiente considerando como punto inicial $x^{1}=10$ y los siguientes valores de tama√±o del paso:   \n",
    "\n",
    "- $\\lambda=0.005$\n",
    "- $\\lambda=0.2$\n",
    "- $\\lambda=0.8$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be875c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertencia: Se alcanz√≥ el n√∫mero m√°ximo de iteraciones (50) sin convergencia.\n"
     ]
    }
   ],
   "source": [
    "xsol1 = metodo_gradiente(xini=10,paso=0.005,n_iter=50,eps=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa82812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergencia alcanzada en iter:  33\n"
     ]
    }
   ],
   "source": [
    "xsol2 = metodo_gradiente(xini=10,paso=0.2,n_iter=50,eps=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980f130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergencia alcanzada en iter:  33\n"
     ]
    }
   ],
   "source": [
    "xsol3 = metodo_gradiente(xini=10,paso=0.8,n_iter=50,eps=1e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7e949",
   "metadata": {},
   "source": [
    "Representamos los resultados usando **plotly**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118d2b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=x**2,\n",
    "                        mode='lines',\n",
    "                        name=r'f(x)=$x^2$',\n",
    "                        line=dict(color=\"yellow\")))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xsol1, y=[x**2 for x in xsol1],\n",
    "                        mode='lines+markers',\n",
    "                        marker=dict(symbol=\"arrow-up\",angleref=\"previous\"),\n",
    "                        name=r'$\\lambda=0.005$'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xsol2, y=[x**2 for x in xsol2],\n",
    "                        mode='lines+markers',\n",
    "                        marker=dict(symbol=\"arrow-up\",angleref=\"previous\"),\n",
    "                        name=r'$\\lambda=0.2$'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xsol3, y=[x**2 for x in xsol3],\n",
    "                        mode='lines+markers',\n",
    "                        marker=dict(symbol=\"arrow-up\",angleref=\"previous\"),\n",
    "                        name=r'$\\lambda=0.8$'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2e437",
   "metadata": {},
   "source": [
    "## **Ejercicios**: \n",
    "- **Ejercicio 1**: prueba a ejecutar la funci√≥n con otros valores de $\\lambda$ (argumento `paso` de la funci√≥n) y ver c√≥mo var√≠a el comportamiento represent√°ndolo gr√°ficamente. Trata de encontrar un valor del paso que converja antes  a la soluci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e642e",
   "metadata": {},
   "source": [
    "- **Ejercicio 2**: Aplica el m√©todo del gradiente a la siguiente funci√≥n:\n",
    "\n",
    "$$\n",
    "\\min_{x_1, x_2}f(x)=(3-x_1)^{2} + 7(x_2-x_1^{2})^{2}\n",
    "$$\n",
    "\n",
    "Ejecuta el algoritmo considerando: $(x_1^{0},x_2^{0})=(10,1)$, $\\lambda^{t}=0.08$, $\\epsilon=10^{-6}$ y n√∫mero m√°ximo de iteraciones 1000. Representa la evoluci√≥n del algoritmo y analiza lo que sucede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3815f8",
   "metadata": {},
   "source": [
    "- **Ejercicio 3**: ¬øC√≥mo var√≠an los resultados si calculamos $\\lambda$ con line search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62e807",
   "metadata": {},
   "source": [
    "# 2. Aplicaci√≥n del M√©todo del Gradiente a Regresi√≥n Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f47c18",
   "metadata": {},
   "source": [
    "Supongamos que tenemos un conjunto de datos de entrenamiento: $(\\pmb{x}^1,y_{1}),\\ldots,(\\pmb{x}^n,y_{n})$, donde $\\pmb{x}^i\\in \\mathbb{R}^{m}$ e $y_{i}\\in \\mathbb{R}$, $i=1,\\ldots,n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2180f9",
   "metadata": {},
   "source": [
    "Entonces, se quiere llevar a cabo un ajuste linear de la forma:\n",
    "$$\n",
    "Y = \\pmb{w}^{T} \\pmb{x}+b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856aca8",
   "metadata": {},
   "source": [
    "La idea es tratar de encontrar los par√°metros $\\pmb{w}$ y $b$ √≥ptimos que minimicen el siguiente error:\n",
    "$$\n",
    "E(\\pmb{w},b)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2}(y_{i}-f(\\pmb{x}^i))^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015efc2",
   "metadata": {},
   "source": [
    "Las derivadas con respecto a cada par√°metro quedar√≠an:\n",
    "$$\n",
    "\\frac{\\partial E(\\pmb{w},b)}{\\partial \\pmb{w_j}}=\\frac{1}{n}\\sum_{i=1}^{n}(f(\\pmb{x}^i)-y_{i})x^{i}_{j}, \\, j=1,\\ldots,m \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E(\\pmb{w},b)}{\\partial b}=\\frac{1}{n}\\sum_{i=1}^{n}(f(\\pmb{x}^i)-y_{i}), \\, j=1,\\ldots,m \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f13f3",
   "metadata": {},
   "source": [
    "Por lo tanto, en cada actualizaci√≥n del m√©todo del gradiente tendr√≠amos:\n",
    "$$\n",
    "\\pmb{w_j}^{t+1} =  \\pmb{w_j}^{t}-\\lambda^{t} \\frac{1}{n}\\sum_{i=1}^{n}(f(\\pmb{x}^i)-y_{i})x^{i}_{j}, \\, j=1,\\ldots,m\n",
    "$$\n",
    "$$\n",
    "b^{t+1} =  b^{t}-\\lambda^{t} \\frac{1}{n}\\sum_{i=1}^{n}(f(\\pmb{x}^i)-y_{i})x^{i}_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e057c6",
   "metadata": {},
   "source": [
    "Supongamos que tenemos el siguiente conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0fcc2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_7.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'\n",
    "\n",
    "# Leemos los datos:\n",
    "data = pd.read_csv('gd_data.csv', header=None)\n",
    "X = data.iloc[:, 0]\n",
    "Y = data.iloc[:, 1]\n",
    "\n",
    "# Representamos un diagrama de dispersi√≥n:\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X, y=Y,\n",
    "                        mode='markers',\n",
    "                        name='Points',\n",
    "                        line=dict(color=\"blue\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a68ac",
   "metadata": {},
   "source": [
    "Implementamos el m√©todo del gradiente para este caso espec√≠fico de **regresi√≥n lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5251dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodo_gradiente_regresion(X,Y,w0,b0,paso,n_iter=50,eps=1e-06):\n",
    "    \"\"\"\n",
    "    Calcula los par√°metros (w, b) de una regresi√≥n lineal simple \n",
    "    (y = wx + b) usando el m√©todo de descenso de gradiente.\n",
    "    \"\"\"\n",
    "    # Aseguramos que X e Y sean arrays de NumPy para operaciones vectorizadas\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    n = len(X)\n",
    "    w = w0\n",
    "    b = b0\n",
    "    for i in range(n_iter):\n",
    "        Ypred = w*X + b                # El valor predicho por el modelo actual\n",
    "        error = Ypred - Y              # Error residual \n",
    "        gradw = (2/n)*np.sum(error*X)  # Gradiente con respecto w\n",
    "        gradb = (2/n)*np.sum(error)    # Gradiente con respecto a b\n",
    "        if (np.abs(gradw))<= eps and (np.abs(gradb)<=eps):\n",
    "            print('Convergencia alcanzada en iter: ',i)\n",
    "            break\n",
    "        # Actualizamos    \n",
    "        w = w-paso*gradw\n",
    "        b = b-paso*gradb\n",
    "\n",
    "    if i == n_iter - 1:\n",
    "        print(f'Advertencia: Se alcanz√≥ el n√∫mero m√°ximo de iteraciones ({n_iter}) sin convergencia.')\n",
    "    \n",
    "    return (w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd83b6",
   "metadata": {},
   "source": [
    "Llevamos a cabo el ajuste para los datos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267afb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertencia: Se alcanz√≥ el n√∫mero m√°ximo de iteraciones (1000) sin convergencia.\n",
      "\n",
      "Resultados finales m√©todo del Gradiente:\n",
      "w (pendiente) = 1.4777\n",
      "b (intercepto) = 0.0889\n"
     ]
    }
   ],
   "source": [
    "(w,b)=metodo_gradiente_regresion(X,Y,w0=0,b0=0,paso=0.0001,n_iter=1000,eps=1e-06)\n",
    "# Valores de los par√°metros obtenidos:\n",
    "print(f\"\\nResultados finales m√©todo del Gradiente:\")\n",
    "print(f\"w (pendiente) = {w:.4f}\")\n",
    "print(f\"b (intercepto) = {b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2aa371-9fc1-47d9-b031-5860703c67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos las predicciones:\n",
    "Y_pred = w*X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e934923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_11.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Puntos para representar la recta ajustada:\n",
    "xplot = np.array([np.min(X),np.max(X)])\n",
    "yplot = w*xplot + b\n",
    "\n",
    "# Representamos recta ajustada con GD\n",
    "fig.add_trace(go.Scatter(x=xplot, y=yplot,\n",
    "                        mode='lines',\n",
    "                        name='GD',\n",
    "                        line=dict(color=\"red\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72b812",
   "metadata": {},
   "source": [
    "### Ajustamos la recta de regresi√≥n lineal de forma exacta (usando sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56613f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "data = pd.read_csv('gd_data.csv', header=None)\n",
    "X = data.iloc[:, 0].values.reshape(-1, 1) # values converts it into a numpy array\n",
    "Y = data.iloc[:, 1].values.reshape(-1, 1) # -1 means that calculate the dimension of rows, but have 1 column\n",
    "linear_regressor = LinearRegression()\n",
    "sol = linear_regressor.fit(X, Y)\n",
    "Y_pred = linear_regressor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15771ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados finales m√©todo del Gradiente:\n",
      "w (pendiente) = 1.3224\n",
      "b (intercepto) = 7.9910\n"
     ]
    }
   ],
   "source": [
    "# Valores de los par√°metros obtenidos con SKLEARN:\n",
    "print(f\"\\nResultados finales m√©todo del Gradiente:\")\n",
    "print(f\"w (pendiente) = {sol.coef_.item():.4f}\")\n",
    "print(f\"b (intercepto) = {sol.intercept_.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dabd9052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_14.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Representamos recta ajustada con GD\n",
    "xplot = np.array([np.min(X),np.max(X)])\n",
    "yplot = sol.intercept_[0] + sol.coef_[0]*xplot\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xplot, y=yplot,\n",
    "                        mode='lines',\n",
    "                        name='Exact',\n",
    "                        line=dict(color=\"green\")))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b2c61-b954-4989-817f-69b67247f938",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid #f0ad4e; padding:10px; border-radius:5px; background-color:#fcf8e3;\">\n",
    "<strong>‚ö†Ô∏è Advertencia: Los resultados obtenidos no coinciden </strong>  <br> \n",
    "</div>\n",
    "  \n",
    "- Notemos que el m√©todo del gradiente no hab√≠a convergido.  \n",
    "- ¬øC√≥mo podemos mejorar su comportamiento?\n",
    "    - Utilizar un paso m√°s peque√±o o m√°s iteraciones.\n",
    "    - **Normalizando los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20de4a2-8c62-4dd3-9955-a098e1bb750f",
   "metadata": {},
   "source": [
    "- **Ejercicio 4**: Calcula los par√°metros empleando el m√©todo del gradiente con los datos normalizados e incluye el resultado en el gr√°fico anterior. **Nota**: emplea un `paso=0.01`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07a6708-b44a-4a13-9b06-709156c59b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los datos:\n",
    "data = pd.read_csv('gd_data.csv', header=None)\n",
    "X = data.iloc[:, 0]\n",
    "Y = data.iloc[:, 1]\n",
    "\n",
    "# Normalizaci√≥n de los datos\n",
    "mean_X = np.mean(X)\n",
    "std_X = np.std(X)\n",
    "\n",
    "mean_Y = np.mean(Y)\n",
    "std_Y = np.std(Y)\n",
    "\n",
    "X_norm = (X - mean_X) / std_X\n",
    "Y_norm = (Y - mean_Y) / std_Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ebdbd",
   "metadata": {},
   "source": [
    "## 2.1 M√©todo de gradiente estoc√°stico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519bf70",
   "metadata": {},
   "source": [
    "El **m√©todo del gradiente estoc√°stico**, en vez de tomar la suma sobre toda la poblaci√≥n, considera una submuestra **$S^{t}\\subset \\{1,\\ldots,n\\}$**. Luego:\n",
    "$$\n",
    "\\pmb{w}^{t+1} = \\pmb{w}^{t}-\\lambda^{t}\\dfrac{1}{|S^{t}|}\\sum_{i\\in S^{t}}\\nabla Q_{i}(\\pmb{w}^{t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83d2a0",
   "metadata": {},
   "source": [
    "### **Ejercicios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cec878",
   "metadata": {},
   "source": [
    "- **Ejercicio 5**: Modifica la funci√≥n \"metodo_gradiente_regresion\" con los cambios necesarios para implementar el **m√©todo del gradiente estoc√°stico**. NOTA: ahora la funci√≥n tendr√° un nuevo argumento de entrada ''tam_muestra'' que denotar√° el tama√±o de las muestras que se quiere considerar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13583deb",
   "metadata": {},
   "source": [
    "- **Ejercicio 6**: Ejecuta el gradiente estoc√°stico sobre los datos 'gd_data.csv' considerando tama√±o de muestra 50. Compara la recta de regresi√≥n obtenida con las obtenidas previamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
